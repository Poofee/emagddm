# 稀疏矩阵模块开发落地计划（2023 年 10 月 2 日）

结合我们此前讨论的**统一框架（串行+区域分解）、低频电磁适配、稀疏矩阵核心需求**，为你制定一套**可直接交给AI执行的稀疏矩阵模块开发落地计划**。计划遵循「从基础到进阶、从串行到分布式、从通用到电磁专用」的逻辑，分6个阶段落地，每个阶段明确**目标、核心任务、技术要求、交付物、测试标准**，AI可按阶段逐步实现，最终完成对标Maxwell的稀疏矩阵能力。

# 稀疏矩阵模块开发落地计划（适配低频电磁有限元+串行/DDM统一框架）

## 一、整体目标

开发一套**C++实现**的稀疏矩阵模块，满足：

1. 基础能力：支持COO/CSR/块稀疏/对称压缩格式，实现核心矩阵操作；

2. 场景适配：兼容低频电磁标量元/矢量元、实/复矩阵、静电/静磁/涡流场；

3. 架构兼容：支持**串行求解**+**区域分解（FETI-DP）** 统一架构，子域/界面/粗空间矩阵无缝衔接；

4. 工程化：提供简洁API、标准I/O、测试用例，可直接集成到有限元框架。

## 二、技术栈约定（AI需严格遵循）

1. 编程语言：C++17（兼顾性能与现代特性）；

2. 依赖：无第三方线性代数库（基础实现），可选集成Eigen（辅助测试），MPI 3.0（分布式/DDM）；

3. 接口规范：抽象基类+继承，函数命名遵循`lower_snake_case`，类名遵循`UpperCamelCase`；

4. 数据格式：支持MatrixMarket格式（.mtx）读写，兼容行业标准。

## 三、分阶段落地计划（按优先级排序，逐步迭代）

### 阶段1：基础稀疏矩阵核心（串行，根基阶段）

#### 阶段目标

实现最核心的**COO/CSR稀疏格式**，完成矩阵构建、格式转换、基础内存管理，为后续所有功能打基础。

#### 核心任务

1. 定义抽象基类`SparseMatrixBase`（纯虚接口，统一矩阵行为）：

    - 纯虚函数：`rows()`、`cols()`、`nnz()`（非零元数）、`clear()`、`print_info()`；

2. 实现COO格式类`CooMatrix`：

    - 成员：行索引数组`row_idx`、列索引数组`col_idx`、数值数组`data`（支持`double`/`std::complex<double>`）；

    - 功能：构造（指定行列/非零元）、添加非零元`add_value()`、批量赋值、内存释放；

3. 实现CSR格式类`CsrMatrix`（继承`SparseMatrixBase`）：

    - 成员：行偏移数组`row_ptr`、列索引数组`col_idx`、数值数组`data`；

    - 功能：构造、添加非零元、内存管理；

4. 实现格式转换：`CooMatrix` ↔ `CsrMatrix` 双向转换（COO适合组装，CSR适合求解）；

5. 基础I/O：实现矩阵读写MatrixMarket格式（.mtx），支持实/复矩阵。

#### 技术要求

1. 内存安全：使用`std::vector`管理动态数组，避免裸指针泄漏；

2. 效率：转换算法时间复杂度O(nnz)，无冗余拷贝；

3. 兼容性：同时支持实数（静电/静磁）、复数（涡流）矩阵。

#### 交付物

1. 头文件：`sparse_base.h`、`coo_matrix.h`、`csr_matrix.h`；

2. 源文件：`sparse_base.cpp`、`coo_matrix.cpp`、`csr_matrix.cpp`；

3. 测试用例：`test_stage1.cpp`（测试矩阵构建、转换、I/O）。

#### 测试验证标准

1. 功能：构建3×3/10×10稀疏矩阵，转换后非零元/数值完全一致；

2. I/O：写入.mtx文件，重新读取后与原矩阵无差异；

3. 边界：空矩阵、全零矩阵、单行/单列矩阵处理无崩溃。

### 阶段2：串行矩阵操作与基础预处理（求解器依赖）

#### 阶段目标

实现串行场景下**核心矩阵操作**（求解器必备）+**基础预处理**，支持直接/迭代法求解。

#### 核心任务

1. 矩阵-向量乘（MV，迭代法核心）：

    - 实现`CsrMatrix::mat_vec(const Vector& x, Vector& y)`（y = A·x）；

    - 支持实/复矩阵，向量`Vector`为基础动态数组类（同步实现）；

2. 矩阵基础操作：

    - 数乘：`scale(double alpha)`（A = α·A）；

    - 转置：`transpose()`（返回转置后的CSR矩阵）；

    - 对角元操作：`get_diag(Vector& diag)`、`set_diag(const Vector& diag)`；

3. 基础预处理生成：

    - Jacobi预处理：`jacobi_precond(CsrMatrix& P)`（P为对角逆矩阵）；

    - ILU(0)预处理：`ilu0_precond(CsrMatrix& L, CsrMatrix& U)`（无填充ILU，适配小规模矩阵）；

4. 对称矩阵优化：

    - 新增`SymCsrMatrix`（对称CSR，仅存下三角），实现对称矩阵的MV、转置（无操作），内存占用减半。

#### 技术要求

1. 性能：MV操作缓存友好，按行遍历CSR，提升CPU缓存命中率；

2. 鲁棒性：预处理时检测对角元为零（电磁矩阵常见），抛出异常或加微小扰动；

3. 复用：`Vector`类与矩阵模块解耦，可独立使用。

#### 交付物

1. 新增文件：`vector.h`、`vector.cpp`、`preconditioner.h`、`preconditioner.cpp`、`sym_csr_matrix.h`、`sym_csr_matrix.cpp`；

2. 测试用例：`test_stage2.cpp`（测试MV、预处理、对称矩阵）。

#### 测试验证标准

1. 正确性：MV结果与稠密矩阵计算结果误差<1e-10；

2. 预处理：ILU(0)分解后，L·U与原矩阵非零模式一致，数值误差<1e-8；

3. 对称矩阵：`SymCsrMatrix`的MV结果与全存储CSR完全一致。

### 阶段3：低频电磁场景适配（对标Maxwell核心特性）

#### 阶段目标

适配低频电磁**矢量元（Nedelec边元）**、**复矩阵（涡流场）**、**块稀疏**特性，实现电磁专用矩阵能力。

#### 核心任务

1. 块稀疏矩阵实现：

    - 新增`BlockCsrMatrix`（块CSR，适配矢量元2×2/3×3块）；

    - 成员：块行偏移`block_row_ptr`、块列索引`block_col_idx`、块数值数组`block_data`；

    - 功能：块级MV、块对角提取、块ILU预处理（适配矢量元）；

2. 复矩阵全功能支持：

    - 所有矩阵类（COO/CSR/块CSR）兼容`std::complex<double>`；

    - 实现复矩阵共轭转置、复MV、复Jacobi/ILU预处理；

3. 电磁矩阵属性标记：

    - 新增`MatrixAttribute`结构体，标记：对称/非对称、正定/半正定、实/复、标量元/矢量元、物理场类型（静电/静磁/涡流）；

    - 矩阵类新增`set_attribute()`/`get_attribute()`，自动适配预处理/操作逻辑；

4. 静磁散度约束适配：

    - 实现`add_div_constraint()`函数，为奇异静磁矩阵添加罚项（修复奇异性）。

#### 技术要求

1. 块操作：块级运算避免单元素循环，提升矢量元计算效率；

2. 复运算：严格遵循复数运算规则，共轭转置正确处理虚部符号；

3. 场景适配：属性标记与预处理自动绑定（如矢量元自动调用块ILU）。

#### 交付物

1. 新增文件：`block_csr_matrix.h`、`block_csr_matrix.cpp`、`matrix_attribute.h`、`em_adapter.h`、`em_adapter.cpp`；

2. 测试用例：`test_stage3.cpp`（测试块矩阵、复矩阵、散度约束）。

#### 测试验证标准

1. 块矩阵：3×3块CSR的MV结果与稠密块矩阵一致；

2. 复矩阵：涡流场复矩阵的共轭转置、MV结果与理论值误差<1e-10；

3. 散度约束：添加罚项后，静磁矩阵从奇异变为非奇异，可正常求解。

### 阶段4：分布式矩阵基础（DDM铺垫，无耦合）

#### 阶段目标

引入MPI，实现**分布式向量/子域矩阵**，完成数据分布、基础通信，为FETI-DP打基础（仅分布式存储，不涉及界面耦合）。

#### 核心任务

1. 分布式向量`DistributedVector`：

    - 成员：局部数据`local_data`、全局大小`global_size`、局部大小`local_size`、MPI通信域`comm`；

    - 功能：局部/全局索引映射、`gather()`（全局汇总）、`scatter()`（全局分发）、`allreduce()`（全局累加）；

2. 分布式稀疏矩阵`DistributedSparseMatrix`：

    - 成员：局部CSR矩阵`local_mat`、全局行列数、子域编号、界面DOF标记；

    - 功能：子域矩阵构建、局部MV、全局非零元统计；

3. MPI通信封装：

    - 实现`MPICommWrapper`类，封装MPI初始化/销毁、进程通信、数据传输，隐藏底层MPI细节；

4. 子域数据分布：

    - 实现简单均分区分布（按行/列分割矩阵），支持1进程（串行）/多进程（分布式）自动切换。

#### 技术要求

1. 兼容性：1进程时，`DistributedVector`/`DistributedSparseMatrix`等价于串行版本；

2. 通信效率：使用MPI非阻塞通信（`Isend`/`Irecv`），减少通信延迟；

3. 解耦：通信模块与矩阵模块独立，可替换MPI实现。

#### 交付物

1. 新增文件：`mpi_wrapper.h`、`mpi_wrapper.cpp`、`distributed_vector.h`、`distributed_vector.cpp`、`distributed_matrix.h`、`distributed_matrix.cpp`；

2. 测试用例：`test_stage4.cpp`（MPI多进程测试分布式向量/矩阵）。

#### 测试验证标准

1. 串行兼容：1进程运行结果与串行代码完全一致；

2. 分布式：2/4进程下，全局MV结果与串行稠密矩阵一致；

3. 通信：`gather`/`scatter`/`allreduce`结果无数据丢失/错误。

### 阶段5：区域分解（FETI-DP）核心矩阵实现（DDM核心）

#### 阶段目标

实现FETI-DP算法必备的**子域矩阵、界面约束矩阵B_i、耦合矩阵F、粗空间矩阵A₀**，完成DDM核心矩阵流程。

#### 核心任务

1. 子域矩阵管理：

    - 实现`SubdomainMatrix`类，封装子域局部矩阵A_i、局部DOF、界面DOF；

    - 支持子域矩阵的局部求解（A_i⁻¹·x，调用MUMPS/SuperLU或内置ILU）；

2. 界面约束矩阵B_i：

    - 实现`InterfaceConstraintMatrix`类，生成子域界面连续性约束（标量元值连续、矢量元切向连续）；

    - 支持块结构B_i（适配矢量元），实现B_i·x、B_iᵀ·y操作；

3. 耦合矩阵F（FETI-DP核心）：

    - 实现`CouplingMatrixF`类，通过分布式计算F = Σ B_iᵀ A_i⁻¹ B_i；

    - 支持F的分布式MV（F·λ），适配GMRES迭代；

4. 粗空间矩阵A₀：

    - 实现`CoarseSpaceMatrix`类，提取子域主自由度（顶点/边），生成全局粗空间矩阵；

    - 支持粗空间矩阵的直接求解（A₀⁻¹·x₀），用于全局校正；

5. DDM矩阵流程封装：

    - 实现`FETIDPMatrixWrapper`类，整合子域、B_i、F、A₀，提供统一的`assemble_ddm()`/`apply_coupling()`接口。

#### 技术要求

1. 分布式计算：F矩阵的组装/ MV完全并行，仅传输界面数据，减少通信量；

2. 电磁适配：矢量元界面约束正确处理切向连续，复矩阵兼容涡流场DDM；

3. 鲁棒性：子域求解时检测奇异矩阵，自动启用散度约束或粗空间校正。

#### 交付物

1. 新增文件：`subdomain_matrix.h`、`subdomain_matrix.cpp`、`interface_constraint.h`、`interface_constraint.cpp`、`coupling_matrix_f.h`、`coupling_matrix_f.cpp`、`coarse_space_matrix.h`、`coarse_space_matrix.cpp`、`fetidp_wrapper.h`、`fetidp_wrapper.cpp`；

2. 测试用例：`test_stage5.cpp`（MPI多进程测试FETI-DP矩阵组装、耦合操作）。

#### 测试验证标准

1. 正确性：FETI-DP矩阵流程结果与串行求解结果误差<1e-10；

2. 分布式：4/8进程下，耦合矩阵F的MV结果与串行一致；

3. 电磁场景：静磁/涡流DDM矩阵可正常组装，无奇异/崩溃。

### 阶段6：集成、优化与工程化（最终落地）

#### 阶段目标

完成稀疏矩阵模块与有限元框架的集成，优化性能，补充工程化能力，达到可使用标准。

#### 核心任务

1. 有限元框架集成：

    - 提供`fe_sparse_adapter.h`，适配有限元单元矩阵组装、DOF映射、边界条件处理；

    - 支持单元矩阵批量写入稀疏矩阵（`add_element_matrix()`）；

2. 性能优化：

    - MV操作启用AVX2指令集优化（针对x86）；

    - 块矩阵、分布式矩阵的内存对齐优化；

    - 非零元过滤（剔除小数值，减少内存占用）；

3. 工程化补充：

    - 日志系统：集成spdlog（可选），输出矩阵信息、错误、性能数据；

    - 配置解析：支持JSON配置文件，指定矩阵格式、预处理类型、DDM参数；

    - Python绑定：用pybind11封装核心接口，支持脚本调用测试；

4. 全流程测试：

    - 电磁标准算例测试：平行板电容（静电）、螺线管电感（静磁）、圆柱涡流（频域）；

    - 性能测试：对比串行/DDM的求解时间、内存占用，对标Maxwell基础性能。

#### 技术要求

1. 集成性：与有限元网格、物理场模块解耦，仅通过接口交互；

2. 性能：大规模矩阵（10万非零元以上）的MV速度比基础版本提升30%以上；

3. 易用性：Python接口简洁，10行代码可完成矩阵构建+求解。

#### 交付物

1. 集成文件：`fe_sparse_adapter.h`、`fe_sparse_adapter.cpp`、`python_bind.cpp`；

2. 工程化文件：`logger.h`、`config.h`、`config.json`；

3. 全流程测试：`test_em_full.cpp`、`test_em.py`；

4. 文档：`API文档.md`、`使用教程.md`。

#### 测试验证标准

1. 集成：可组装有限元单元矩阵，求解电磁算例，结果与理论值/Maxwell一致；

2. 性能：DDM模式下，8进程加速比>5（线性加速）；

3. 易用性：Python脚本可正常调用，无编译/运行错误。

## 四、AI开发注意事项

1. 阶段依赖：严格按阶段1→6执行，前一阶段测试通过后再启动下一阶段；

2. 模块化：每个功能独立成类/函数，避免代码耦合，方便后续修改；

3. 测试驱动：每个功能实现后立即编写测试用例，确保正确性；

4. 电磁优先：所有矩阵操作优先适配低频电磁场景（矢量元、复矩阵、散度约束）；

5. 统一框架：始终遵循「串行=1进程DDM」的设计，不拆分串行/DDM代码。

## 五、后续扩展（预留方向，AI可后续实现）

1. 高级预处理：矢量元AMG、BDDC预处理；

2. GPU加速：集成cuSPARSE，实现GPU端MV/预处理；

3. 自适应矩阵：支持网格自适应后的矩阵动态更新；

4. 更多格式：支持HB、CSC格式，兼容更多第三方求解器。
> （注：文档部分内容可能由 AI 生成）